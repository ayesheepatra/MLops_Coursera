{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mATctWQOzNPY"
   },
   "source": [
    "# Ungraded Lab: Walkthrough of ML Metadata\n",
    "\n",
    "Keeping records at each stage of the project is an important aspect of machine learning pipelines. Especially in production models which involve many iterations of datasets and re-training, having these records will help in maintaining or debugging the deployed system. [ML Metadata](https://www.tensorflow.org/tfx/guide/mlmd) addresses this need by having an API suited specifically for keeping track of any progress made in ML projects.\n",
    "\n",
    "As mentioned in earlier labs, you have already used ML Metadata when you ran your TFX pipelines. Each component automatically records information to a metadata store as you go through each stage. It allowed you to retrieve information such as the name of the training splits or the location of an inferred schema. \n",
    "\n",
    "In this notebook, you will look more closely at how ML Metadata can be used directly for recording and retrieving metadata independent from a TFX pipeline (i.e. without using TFX components). You will use TFDV to infer a schema and record all information about this process. These will show how the different components are related to each other so you can better interact with the database when you go back to using TFX in the next labs. Moreover, knowing the inner workings of the library will help you adapt it for other platforms if needed.\n",
    "\n",
    "Let's get to it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVy_Apm8w6yV"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WkuOLGILwQWz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version: 2.3.1\n",
      "TFDV version: 0.24.1\n"
     ]
    }
   ],
   "source": [
    "from ml_metadata.metadata_store import metadata_store\n",
    "from ml_metadata.proto import metadata_store_pb2\n",
    "\n",
    "import tensorflow as tf\n",
    "print('TF version: {}'.format(tf.__version__))\n",
    "\n",
    "import tensorflow_data_validation as tfdv\n",
    "print('TFDV version: {}'.format(tfdv.version.__version__))\n",
    "\n",
    "import urllib\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uc_fsssgxphf"
   },
   "source": [
    "## Download dataset\n",
    "\n",
    "You will be using the [Chicago Taxi](https://data.cityofchicago.org/Transportation/Taxi-Trips/wrvz-psew) dataset for this lab. Let's download the CSVs into your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "9fBeyZydxe8J"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's what we downloaded:\n",
      "\u001b[1m\u001b[36meval\u001b[m\u001b[m    \u001b[1m\u001b[36mserving\u001b[m\u001b[m \u001b[1m\u001b[36mtrain\u001b[m\u001b[m\n",
      "\n",
      "data/eval:\n",
      "data.csv\n",
      "\n",
      "data/serving:\n",
      "data.csv\n",
      "\n",
      "data/train:\n",
      "data.csv\n"
     ]
    }
   ],
   "source": [
    "# Download the zip file from GCP and unzip it\n",
    "url = 'https://storage.googleapis.com/artifacts.tfx-oss-public.appspot.com/datasets/chicago_data.zip'\n",
    "zip, headers = urllib.request.urlretrieve(url)\n",
    "zipfile.ZipFile(zip).extractall()\n",
    "zipfile.ZipFile(zip).close()\n",
    "\n",
    "print(\"Here's what we downloaded:\")\n",
    "!ls -R data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Outline\n",
    "\n",
    "Here is the figure shown in class that describes the different components in an ML Metadata store:\n",
    "\n",
    "<img src='images/mlmd_overview.png' alt='image of mlmd overview'>\n",
    "\n",
    "The green box in the middle shows the data model followed by ML Metadata. The [official documentation](https://www.tensorflow.org/tfx/guide/mlmd#data_model) describe each of these and we'll show it here as well for easy reference:\n",
    "\n",
    "* `ArtifactType` describes an artifact's type and its properties that are stored in the metadata store. You can register these types on-the-fly with the metadata store in code, or you can load them in the store from a serialized format. Once you register a type, its definition is available throughout the lifetime of the store.\n",
    "* An `Artifact` describes a specific instance of an ArtifactType, and its properties that are written to the metadata store.\n",
    "* An `ExecutionType` describes a type of component or step in a workflow, and its runtime parameters.\n",
    "* An `Execution` is a record of a component run or a step in an ML workflow and the runtime parameters. An execution can be thought of as an instance of an ExecutionType. Executions are recorded when you run an ML pipeline or step.\n",
    "* An `Event` is a record of the relationship between artifacts and executions. When an execution happens, events record every artifact that was used by the execution, and every artifact that was produced. These records allow for lineage tracking throughout a workflow. By looking at all events, MLMD knows what executions happened and what artifacts were created as a result. MLMD can then recurse back from any artifact to all of its upstream inputs.\n",
    "* A `ContextType` describes a type of conceptual group of artifacts and executions in a workflow, and its structural properties. For example: projects, pipeline runs, experiments, owners etc.\n",
    "* A `Context` is an instance of a ContextType. It captures the shared information within the group. For example: project name, changelist commit id, experiment annotations etc. It has a user-defined unique name within its ContextType.\n",
    "* An `Attribution` is a record of the relationship between artifacts and contexts.\n",
    "* An `Association` is a record of the relationship between executions and contexts.\n",
    "\n",
    "As mentioned earlier, you will use TFDV to generate a schema and record this process in the ML Metadata store. You will be starting from scratch so you will be defining each component of the data model. The outline of steps involve:\n",
    "\n",
    "1. Defining the ML Metadata's storage database\n",
    "1. Setting up the necessary artifact types\n",
    "1. Setting up the execution types\n",
    "1. Generating an input artifact unit\n",
    "1. Generating an execution unit\n",
    "1. Registering an input event\n",
    "1. Running the TFDV component\n",
    "1. Generating an output artifact unit\n",
    "1. Registering an output event\n",
    "1. Updating the execution unit\n",
    "1. Seting up and generating a context unit\n",
    "1. Generating attributions and associations\n",
    "\n",
    "You can then retrieve information from the database to investigate aspects of your project. For example, you can find which dataset was used to generate a particular schema. You will also do that in this exercise.\n",
    "\n",
    "For each of these steps, you may want to have the [MetadataStore API documentation](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore) open so you can lookup any of the methods you will be using to interact with the metadata store. You can also look at the `metadata_store` protocol buffer [here](https://github.com/google/ml-metadata/blob/r0.24.0/ml_metadata/proto/metadata_store.proto) to see descriptions of each data type covered in this tutorial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSeEtc-N09lG"
   },
   "source": [
    "## Define ML Metadata's Storage Database\n",
    "\n",
    "The first step would be to instantiate your storage backend. As mentioned in class, there are several types supported such as fake (temporary) database, SQLite, MySQL, and even cloud-based storage. For this demo, you will just be using a fake database for quick experimentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "YPDflax61NoK"
   },
   "outputs": [],
   "source": [
    "# Instantiate a connection config\n",
    "connection_config = metadata_store_pb2.ConnectionConfig()\n",
    "\n",
    "# Set an empty fake database proto\n",
    "connection_config.fake_database.SetInParent() \n",
    "\n",
    "# Setup the metadata store\n",
    "store = metadata_store.MetadataStore(connection_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WoPq04D9y-Wa"
   },
   "source": [
    "## Register ArtifactTypes\n",
    "\n",
    "Next, you will create the artifact types needed and register them to the store. Since our simple exercise will just involve generating a schema using TFDV, you will only create two artifact types: one for the **input dataset** and another for the **output schema**. The main steps will be to:\n",
    "\n",
    "* Declare an `ArtifactType()`\n",
    "* Define the name of the artifact type\n",
    "* Define the necessary properties within these artifact types. For example, it is important to know the data split name so you may want to have a `split` property for the artifact type that holds datasets.\n",
    "* Use `put_artifact_type()` to register them to the metadata store. This generates an `id` that you can use later to refer to a particular artifact type.\n",
    "\n",
    "*Bonus: For practice, you can also extend the code below to create an artifact type for the statistics.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "6EOZdczwygJw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data artifact type:\n",
      " name: \"DataSet\"\n",
      "properties {\n",
      "  key: \"name\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"split\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "\n",
      "Schema artifact type:\n",
      " name: \"Schema\"\n",
      "properties {\n",
      "  key: \"name\"\n",
      "  value: STRING\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value: INT\n",
      "}\n",
      "\n",
      "Data artifact type ID: 1\n",
      "Schema artifact type ID: 2\n"
     ]
    }
   ],
   "source": [
    "# Create ArtifactType for the input dataset\n",
    "data_artifact_type = metadata_store_pb2.ArtifactType()\n",
    "data_artifact_type.name = 'DataSet'\n",
    "data_artifact_type.properties['name'] = metadata_store_pb2.STRING\n",
    "data_artifact_type.properties['split'] = metadata_store_pb2.STRING\n",
    "data_artifact_type.properties['version'] = metadata_store_pb2.INT\n",
    "\n",
    "# Register artifact type to the Metadata Store\n",
    "data_artifact_type_id = store.put_artifact_type(data_artifact_type)\n",
    "\n",
    "# Create ArtifactType for Schema\n",
    "schema_artifact_type = metadata_store_pb2.ArtifactType()\n",
    "schema_artifact_type.name = 'Schema'\n",
    "schema_artifact_type.properties['name'] = metadata_store_pb2.STRING\n",
    "schema_artifact_type.properties['version'] = metadata_store_pb2.INT\n",
    "\n",
    "# Register artifact type to the Metadata Store\n",
    "schema_artifact_type_id = store.put_artifact_type(schema_artifact_type)\n",
    "\n",
    "print('Data artifact type:\\n', data_artifact_type)\n",
    "print('Schema artifact type:\\n', schema_artifact_type)\n",
    "print('Data artifact type ID:', data_artifact_type_id)\n",
    "print('Schema artifact type ID:', schema_artifact_type_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqvLdewv1tFP"
   },
   "source": [
    "## Register ExecutionType\n",
    "\n",
    "You will then create the execution types needed. For the simple setup, you will just declare one for the data validation component with a `state` property so you can record if the process is running or already completed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "jRSjo5hJ1lI0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation execution type:\n",
      " name: \"Data Validation\"\n",
      "properties {\n",
      "  key: \"state\"\n",
      "  value: STRING\n",
      "}\n",
      "\n",
      "Data validation execution type ID: 3\n"
     ]
    }
   ],
   "source": [
    "# Create ExecutionType for Data Validation component\n",
    "dv_execution_type = metadata_store_pb2.ExecutionType()\n",
    "dv_execution_type.name = 'Data Validation'\n",
    "dv_execution_type.properties['state'] = metadata_store_pb2.STRING\n",
    "\n",
    "# Register execution type to the Metadata Store\n",
    "dv_execution_type_id = store.put_execution_type(dv_execution_type)\n",
    "\n",
    "print('Data validation execution type:\\n', dv_execution_type)\n",
    "print('Data validation execution type ID:', dv_execution_type_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lY6su4si2YlA"
   },
   "source": [
    "## Generate input artifact unit\n",
    "\n",
    "With the artifact types created, you can now create instances of those types. The cell below creates the artifact for the input dataset. This artifact is recorded in the metadata store through the `put_artifacts()` function. Again, it generates an `id` that can be used for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ikhYWam82O9r"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data artifact:\n",
      " type_id: 1\n",
      "uri: \"./data/train/data.csv\"\n",
      "properties {\n",
      "  key: \"name\"\n",
      "  value {\n",
      "    string_value: \"Chicago Taxi dataset\"\n",
      "  }\n",
      "}\n",
      "properties {\n",
      "  key: \"split\"\n",
      "  value {\n",
      "    string_value: \"train\"\n",
      "  }\n",
      "}\n",
      "properties {\n",
      "  key: \"version\"\n",
      "  value {\n",
      "    int_value: 1\n",
      "  }\n",
      "}\n",
      "\n",
      "Data artifact ID: 2\n"
     ]
    }
   ],
   "source": [
    "# Declare input artifact of type DataSet\n",
    "data_artifact = metadata_store_pb2.Artifact()\n",
    "data_artifact.uri = './data/train/data.csv'\n",
    "data_artifact.type_id = data_artifact_type_id\n",
    "data_artifact.properties['name'].string_value = 'Chicago Taxi dataset'\n",
    "data_artifact.properties['split'].string_value = 'train'\n",
    "data_artifact.properties['version'].int_value = 1\n",
    "\n",
    "# Submit input artifact to the Metadata Store\n",
    "data_artifact_id = store.put_artifacts([data_artifact])[0]\n",
    "\n",
    "print('Data artifact:\\n', data_artifact)\n",
    "print('Data artifact ID:', data_artifact_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vTvyxfgY3Mau"
   },
   "source": [
    "## Generate execution unit\n",
    "\n",
    "Next, you will create an instance of the `Data Validation` execution type you registered earlier. You will set the state to `RUNNING` to signify that you are about to run the TFDV function. This is recorded with the `put_executions()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "6wT_55MG28Y3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data validation execution:\n",
      " type_id: 3\n",
      "properties {\n",
      "  key: \"state\"\n",
      "  value {\n",
      "    string_value: \"RUNNING\"\n",
      "  }\n",
      "}\n",
      "\n",
      "Data validation execution ID: 1\n"
     ]
    }
   ],
   "source": [
    "# Register the Execution of a Data Validation run\n",
    "dv_execution = metadata_store_pb2.Execution()\n",
    "dv_execution.type_id = dv_execution_type_id\n",
    "dv_execution.properties['state'].string_value = 'RUNNING'\n",
    "\n",
    "# Submit execution unit to the Metadata Store\n",
    "dv_execution_id = store.put_executions([dv_execution])[0]\n",
    "\n",
    "print('Data validation execution:\\n', dv_execution)\n",
    "print('Data validation execution ID:', dv_execution_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5cbE0xFJ4GV-"
   },
   "source": [
    "## Register input event\n",
    "\n",
    "An event defines a relationship between artifacts and executions. You will generate the input event relationship for dataset artifact and data validation execution units. The list of event types are shown [here](https://github.com/google/ml-metadata/blob/master/ml_metadata/proto/metadata_store.proto#L187) and the event is recorded with the `put_events()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "bUtM4uql29k5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input event:\n",
      " artifact_id: 2\n",
      "execution_id: 1\n",
      "type: DECLARED_INPUT\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Declare the input event\n",
    "input_event = metadata_store_pb2.Event()\n",
    "input_event.artifact_id = data_artifact_id\n",
    "input_event.execution_id = dv_execution_id\n",
    "input_event.type = metadata_store_pb2.Event.DECLARED_INPUT\n",
    "\n",
    "# Submit input event to the Metadata Store\n",
    "store.put_events([input_event])\n",
    "\n",
    "print('Input event:\\n', input_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bkiMylXj4tOJ"
   },
   "source": [
    "## Run the TFDV component\n",
    "\n",
    "You will now run the TFDV component to generate the schema of dataset. This should look familiar since you've done this already in Week 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "_bNnuYbO4q6b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:apache_beam.runners.interactive.interactive_environment:Dependencies required for Interactive Beam PCollection visualization are not available, please use: `pip install apache-beam[interactive]` to install necessary dependencies to enable all data visualization features.\n"
     ]
    },
    {
     "data": {
      "application/javascript": "\n        if (typeof window.interactive_beam_jquery == 'undefined') {\n          var jqueryScript = document.createElement('script');\n          jqueryScript.src = 'https://code.jquery.com/jquery-3.4.1.slim.min.js';\n          jqueryScript.type = 'text/javascript';\n          jqueryScript.onload = function() {\n            var datatableScript = document.createElement('script');\n            datatableScript.src = 'https://cdn.datatables.net/1.10.20/js/jquery.dataTables.min.js';\n            datatableScript.type = 'text/javascript';\n            datatableScript.onload = function() {\n              window.interactive_beam_jquery = jQuery.noConflict(true);\n              window.interactive_beam_jquery(document).ready(function($){\n                \n              });\n            }\n            document.head.appendChild(datatableScript);\n          };\n          document.head.appendChild(jqueryScript);\n        } else {\n          window.interactive_beam_jquery(document).ready(function($){\n            \n          });\n        }"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Make sure that locally built Python SDK docker image has Python 3.6 interpreter.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "tensorflow.python.framework.errors_impl.NotFoundError: No attr named 'BoostedTreesFlushQuantileSummaries' in NodeDef:\n\t [[{{node quantiles_combiner}}]] [while running 'GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/WindowIntoDiscarding']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common._OutputProcessor.process_outputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.SingletonConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.PGBKCVOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.PGBKCVOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36madd_input\u001b[0;34m(self, accumulator, input_record_batch)\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_byte_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtable_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTotalByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_record_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_do_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_instances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m_maybe_do_batch\u001b[0;34m(self, accumulator, force)\u001b[0m\n\u001b[1;32m    671\u001b[0m           \u001b[0;32mlambda\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_acc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           accumulator.partial_accumulators)\n\u001b[0m\u001b[1;32m    673\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_record_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m_for_each_generator\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    625\u001b[0m     return [func(gen, *args_for_func) for gen, args_for_func in zip(\n\u001b[0;32m--> 626\u001b[0;31m         self._generators, zip(*args))]\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m     return [func(gen, *args_for_func) for gen, args_for_func in zip(\n\u001b[0m\u001b[1;32m    626\u001b[0m         self._generators, zip(*args))]\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gen, gen_acc)\u001b[0m\n\u001b[1;32m    670\u001b[0m       accumulator.partial_accumulators = self._for_each_generator(\n\u001b[0;32m--> 671\u001b[0;31m           \u001b[0;32mlambda\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_acc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m           accumulator.partial_accumulators)\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/generators/basic_stats_generator.py\u001b[0m in \u001b[0;36madd_input\u001b[0;34m(self, accumulator, examples)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         stats_for_feature.numeric_stats.quantiles_summary = (\n\u001b[0;32m-> 1053\u001b[0;31m             self._values_quantiles_combiner.create_accumulator())\n\u001b[0m\u001b[1;32m   1054\u001b[0m         stats_for_feature.numeric_stats.weighted_quantiles_summary = (\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/utils/quantiles_util.py\u001b[0m in \u001b[0;36mcreate_accumulator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quantiles_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36mcreate_accumulator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2029\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2030\u001b[0;31m     \u001b[0mgraph_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2031\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36m_get_graph_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2025\u001b[0m       self._graph_state = _QuantilesGraphStateProvider.get_graph_state(\n\u001b[0;32m-> 2026\u001b[0;31m           graph_state_options)\n\u001b[0m\u001b[1;32m   2027\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36mget_graph_state\u001b[0;34m(cls, graph_state_options)\u001b[0m\n\u001b[1;32m   2355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_QuantilesGraphState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_state_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m       \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_states_by_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_state_options\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m   2213\u001b[0m               \u001b[0mquantile_stream_resource_handle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2214\u001b[0;31m               num_features=options.num_features))\n\u001b[0m\u001b[1;32m   2215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mmake_callable\u001b[0;34m(self, fetches, feed_list, accept_options)\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;31m# in the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: No attr named 'BoostedTreesFlushQuantileSummaries' in NodeDef:\n\t [[{{node quantiles_combiner}}]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-13b1a8f92152>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Infer a schema by passing statistics to `infer_schema()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./data/train/data.csv'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_stats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_statistics_from_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mschema\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtfdv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer_schema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatistics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_stats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/utils/stats_gen_lib.py\u001b[0m in \u001b[0;36mgenerate_statistics_from_csv\u001b[0;34m(data_location, column_names, delimiter, output_path, stats_options, pipeline_options, compression_type)\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;34m|\u001b[0m \u001b[0;34m'GenerateStatistics'\u001b[0m \u001b[0;34m>>\u001b[0m \u001b[0mstats_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGenerateStatistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstats_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         | 'WriteStatsOutput' >> stats_api.WriteStatisticsToTFRecord(\n\u001b[0;32m--> 191\u001b[0;31m             output_path))\n\u001b[0m\u001b[1;32m    192\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mstats_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_statistics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    594\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_until_finish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/pipeline.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, test_runner_api)\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m           \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmpdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_in_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/direct/direct_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    129\u001b[0m       \u001b[0mrunner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBundleBasedDirectRunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mrun_pipeline\u001b[0;34m(self, pipeline, options)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     self._latest_run_result = self.run_via_runner_api(\n\u001b[0;32m--> 196\u001b[0;31m         pipeline.to_runner_api(default_environment=self._default_environment))\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_latest_run_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mrun_via_runner_api\u001b[0;34m(self, pipeline_proto)\u001b[0m\n\u001b[1;32m    204\u001b[0m     \u001b[0;31m# TODO(pabloem, BEAM-7514): Create a watermark manager (that has access to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;31m#   the teststream (if any), and all the stages).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_stages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_context\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mcontextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mrun_stages\u001b[0;34m(self, stage_context, stages)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m           stage_results = self._run_stage(\n\u001b[0;32m--> 385\u001b[0;31m               runner_execution_context, bundle_context_manager)\n\u001b[0m\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m           assert (\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36m_run_stage\u001b[0;34m(self, runner_execution_context, bundle_context_manager)\u001b[0m\n\u001b[1;32m    651\u001b[0m               \u001b[0minput_timers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m               \u001b[0mexpected_timer_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m               bundle_manager))\n\u001b[0m\u001b[1;32m    654\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mpc_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatermark\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwatermark_updates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36m_run_bundle\u001b[0;34m(self, runner_execution_context, bundle_context_manager, data_input, data_output, input_timers, expected_timer_output, bundle_manager)\u001b[0m\n\u001b[1;32m    768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m     result, splits = bundle_manager.process_bundle(\n\u001b[0;32m--> 770\u001b[0;31m         data_input, data_output, input_timers, expected_timer_output)\n\u001b[0m\u001b[1;32m    771\u001b[0m     \u001b[0;31m# Now we collect all the deferred inputs remaining from bundle execution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[0;31m# Deferred inputs can be:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/portability/fn_api_runner/fn_runner.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, inputs, expected_outputs, fired_timers, expected_output_timers, dry_run)\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mprocess_bundle_descriptor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1079\u001b[0m             cache_tokens=[next(self._cache_token_generator)]))\n\u001b[0;32m-> 1080\u001b[0;31m     \u001b[0mresult_future\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_worker_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_bundle_req\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1082\u001b[0m     \u001b[0msplit_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# type: List[beam_fn_api_pb2.ProcessBundleSplitResponse]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/portability/fn_api_runner/worker_handlers.py\u001b[0m in \u001b[0;36mpush\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    376\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uid_counter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m       \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'control_%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uid_counter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mworker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdo_instruction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mControlFuture\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/sdk_worker.py\u001b[0m in \u001b[0;36mdo_instruction\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;31m# E.g. if register is set, this will call self.register(request.register))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m       return getattr(self, request_type)(\n\u001b[0;32m--> 598\u001b[0;31m           getattr(request, request_type), request.instruction_id)\n\u001b[0m\u001b[1;32m    599\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/sdk_worker.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, request, instruction_id)\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_profile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstruction_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m           delayed_applications, requests_finalization = (\n\u001b[0;32m--> 635\u001b[0;31m               bundle_processor.process_bundle(instruction_id))\n\u001b[0m\u001b[1;32m    636\u001b[0m           \u001b[0mmonitoring_infos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbundle_processor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmonitoring_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m           \u001b[0mmonitoring_infos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_cache_metrics_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/bundle_processor.py\u001b[0m in \u001b[0;36mprocess_bundle\u001b[0;34m(self, instruction_id)\u001b[0m\n\u001b[1;32m    994\u001b[0m           \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeam_fn_api_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElements\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m             input_op_by_transform_id[element.transform_id].process_encoded(\n\u001b[0;32m--> 996\u001b[0;31m                 element.data)\n\u001b[0m\u001b[1;32m    997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m       \u001b[0;31m# Finish all operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/bundle_processor.py\u001b[0m in \u001b[0;36mprocess_encoded\u001b[0;34m(self, encoded_windowed_values)\u001b[0m\n\u001b[1;32m    219\u001b[0m       decoded_value = self.windowed_coder_impl.decode_from_stream(\n\u001b[1;32m    220\u001b[0m           input_stream, True)\n\u001b[0;32m--> 221\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoded_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mmonitoring_infos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_to_pcollection_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.Operation.output\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.SingletonConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common._OutputProcessor.process_outputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.SingletonConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common._OutputProcessor.process_outputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.ConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.SimpleInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common._OutputProcessor.process_outputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.SingletonConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.SingletonConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.DoOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner._reraise_augmented\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.DoFnRunner.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker.invoke_process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common.PerWindowInvoker._invoke_process_per_window\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/common.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.common._OutputProcessor.process_outputs\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.SingletonConsumerSet.receive\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.PGBKCVOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/apache_beam/runners/worker/operations.cpython-36m-darwin.so\u001b[0m in \u001b[0;36mapache_beam.runners.worker.operations.PGBKCVOperation.process\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36madd_input\u001b[0;34m(self, accumulator, input_record_batch)\u001b[0m\n\u001b[1;32m    683\u001b[0m     \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_batch_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnum_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m     \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_byte_size\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtable_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTotalByteSize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_record_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 685\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_do_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccumulator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    686\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_instances\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m_maybe_do_batch\u001b[0;34m(self, accumulator, force)\u001b[0m\n\u001b[1;32m    670\u001b[0m       accumulator.partial_accumulators = self._for_each_generator(\n\u001b[1;32m    671\u001b[0m           \u001b[0;32mlambda\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_acc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m           accumulator.partial_accumulators)\n\u001b[0m\u001b[1;32m    673\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_record_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m       \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurr_batch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m_for_each_generator\u001b[0;34m(self, func, *args)\u001b[0m\n\u001b[1;32m    624\u001b[0m     \"\"\"\n\u001b[1;32m    625\u001b[0m     return [func(gen, *args_for_func) for gen, args_for_func in zip(\n\u001b[0;32m--> 626\u001b[0;31m         self._generators, zip(*args))]\n\u001b[0m\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m   def create_accumulator(self\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    623\u001b[0m       \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generators\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m...\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m     \"\"\"\n\u001b[0;32m--> 625\u001b[0;31m     return [func(gen, *args_for_func) for gen, args_for_func in zip(\n\u001b[0m\u001b[1;32m    626\u001b[0m         self._generators, zip(*args))]\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/stats_impl.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(gen, gen_acc)\u001b[0m\n\u001b[1;32m    669\u001b[0m             accumulator.input_record_batches)\n\u001b[1;32m    670\u001b[0m       accumulator.partial_accumulators = self._for_each_generator(\n\u001b[0;32m--> 671\u001b[0;31m           \u001b[0;32mlambda\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_acc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecord_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m           accumulator.partial_accumulators)\n\u001b[1;32m    673\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0maccumulator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_record_batches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/statistics/generators/basic_stats_generator.py\u001b[0m in \u001b[0;36madd_input\u001b[0;34m(self, accumulator, examples)\u001b[0m\n\u001b[1;32m   1051\u001b[0m         \u001b[0;31m# Store empty summary for each of the quantiles computations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m         stats_for_feature.numeric_stats.quantiles_summary = (\n\u001b[0;32m-> 1053\u001b[0;31m             self._values_quantiles_combiner.create_accumulator())\n\u001b[0m\u001b[1;32m   1054\u001b[0m         stats_for_feature.numeric_stats.weighted_quantiles_summary = (\n\u001b[1;32m   1055\u001b[0m             self._values_quantiles_combiner.create_accumulator())\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_data_validation/utils/quantiles_util.py\u001b[0m in \u001b[0;36mcreate_accumulator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_quantiles_spec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m   def add_input(\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36mcreate_accumulator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2029\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcreate_accumulator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2030\u001b[0;31m     \u001b[0mgraph_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_graph_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2031\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_summary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36m_get_graph_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2024\u001b[0m           random_slot=random_slot)\n\u001b[1;32m   2025\u001b[0m       self._graph_state = _QuantilesGraphStateProvider.get_graph_state(\n\u001b[0;32m-> 2026\u001b[0;31m           graph_state_options)\n\u001b[0m\u001b[1;32m   2027\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36mget_graph_state\u001b[0;34m(cls, graph_state_options)\u001b[0m\n\u001b[1;32m   2354\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_states_by_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_state_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2356\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_QuantilesGraphState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgraph_state_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2357\u001b[0m       \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_states_by_options\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgraph_state_options\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2358\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow_transform/analyzers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options)\u001b[0m\n\u001b[1;32m   2212\u001b[0m           fetches=tf.raw_ops.BoostedTreesFlushQuantileSummaries(\n\u001b[1;32m   2213\u001b[0m               \u001b[0mquantile_stream_resource_handle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2214\u001b[0;31m               num_features=options.num_features))\n\u001b[0m\u001b[1;32m   2215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2216\u001b[0m       \u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mmake_callable\u001b[0;34m(self, fetches, feed_list, accept_options)\u001b[0m\n\u001b[1;32m   1239\u001b[0m     \u001b[0;31m# returned object, because the arguments to `fetches` must already be\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1240\u001b[0m     \u001b[0;31m# in the graph.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1241\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1243\u001b[0m     \u001b[0;31m# Create a fetch handler to take care of the structure of fetches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/envs/mlops_tf_coursera/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1386\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: tensorflow.python.framework.errors_impl.NotFoundError: No attr named 'BoostedTreesFlushQuantileSummaries' in NodeDef:\n\t [[{{node quantiles_combiner}}]] [while running 'GenerateStatistics/RunStatsGenerators/GenerateSlicedStatisticsImpl/RunCombinerStatsGenerators/WindowIntoDiscarding']"
     ]
    }
   ],
   "source": [
    "# Infer a schema by passing statistics to `infer_schema()`\n",
    "train_data = './data/train/data.csv'\n",
    "train_stats = tfdv.generate_statistics_from_csv(data_location=train_data)\n",
    "schema = tfdv.infer_schema(statistics=train_stats)\n",
    "\n",
    "schema_file = './schema.pbtxt'\n",
    "tfdv.write_schema_text(schema, schema_file)\n",
    "\n",
    "print(\"Dataset's Schema has been generated at:\", schema_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JnlO_oLN6KE7"
   },
   "source": [
    "## Generate output artifact unit\n",
    "\n",
    "Now that the TFDV component has finished running and schema has been generated, you can create the artifact for the generated schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KZRphapZ57Op"
   },
   "outputs": [],
   "source": [
    "# Declare output artifact of type Schema_artifact\n",
    "schema_artifact = metadata_store_pb2.Artifact()\n",
    "schema_artifact.uri = schema_file\n",
    "schema_artifact.type_id = schema_artifact_type_id\n",
    "schema_artifact.properties['version'].int_value = 1\n",
    "schema_artifact.properties['name'].string_value = 'Chicago Taxi Schema'\n",
    "\n",
    "# Submit output artifact to the Metadata Store\n",
    "schema_artifact_id = store.put_artifacts([schema_artifact])[0]\n",
    "\n",
    "print('Schema artifact:\\n', schema_artifact)\n",
    "print('Schema artifact ID:', schema_artifact_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zf75jWWwBo_w"
   },
   "source": [
    "## Register output event\n",
    "\n",
    "Analogous to the input event earlier, you also want to define an output event to record the ouput artifact of a particular execution unit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucjBtFzwBPFL"
   },
   "outputs": [],
   "source": [
    "# Declare the output event\n",
    "output_event = metadata_store_pb2.Event()\n",
    "output_event.artifact_id = schema_artifact_id\n",
    "output_event.execution_id = dv_execution_id\n",
    "output_event.type = metadata_store_pb2.Event.DECLARED_OUTPUT\n",
    "\n",
    "# Submit output event to the Metadata Store\n",
    "store.put_events([output_event])\n",
    "\n",
    "print('Output event:\\n', output_event)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phwyb50xB0mU"
   },
   "source": [
    "## Update the execution unit\n",
    "\n",
    "As the TFDV component has finished running successfully, you need to update the `state` of the execution unit and record it again to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UvFZuNWhByqS"
   },
   "outputs": [],
   "source": [
    "# Mark the `state` as `COMPLETED`\n",
    "dv_execution.id = dv_execution_id\n",
    "dv_execution.properties['state'].string_value = 'COMPLETED'\n",
    "\n",
    "# Update execution unit in the Metadata Store\n",
    "store.put_executions([dv_execution])\n",
    "\n",
    "print('Data validation execution:\\n', dv_execution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdwwpejkDEvA"
   },
   "source": [
    "## Setting up Context Types and Generating a Context Unit\n",
    "\n",
    "You can group the artifacts and execution units into a `Context`. First, you need to define a `ContextType` which defines the required context. It follows a similar format as artifact and event types. You can register this with the `put_context_type()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U2cdSos6DDyL"
   },
   "outputs": [],
   "source": [
    "# Create a ContextType\n",
    "expt_context_type = metadata_store_pb2.ContextType()\n",
    "expt_context_type.name = 'Experiment'\n",
    "expt_context_type.properties['note'] = metadata_store_pb2.STRING\n",
    "\n",
    "# Register context type to the Metadata Store\n",
    "expt_context_type_id = store.put_context_type(expt_context_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, you can create an instance of this context type and use the `put_contexts()` method to register to the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the context\n",
    "expt_context = metadata_store_pb2.Context()\n",
    "expt_context.type_id = expt_context_type_id\n",
    "# Give the experiment a name\n",
    "expt_context.name = 'Demo'\n",
    "expt_context.properties['note'].string_value = 'Walkthrough of metadata'\n",
    "\n",
    "# Submit context to the Metadata Store\n",
    "expt_context_id = store.put_contexts([expt_context])[0]\n",
    "\n",
    "print('Experiment Context type:\\n', expt_context_type)\n",
    "print('Experiment Context type ID: ', expt_context_type_id)\n",
    "\n",
    "print('Experiment Context:\\n', expt_context)\n",
    "print('Experiment Context ID: ', expt_context_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtLdOxk1Egte"
   },
   "source": [
    "## Generate attribution and association relationships\n",
    "\n",
    "With the `Context` defined, you can now create its relationship with the artifact and executions you previously used. You will create the relationship between schema artifact unit and experiment context unit to form an `Attribution`.\n",
    "Similarly, you will create the relationship between data validation execution unit and experiment context unit to form an `Association`. These are registered with the `put_attributions_and_associations()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E75AIntwCzpW"
   },
   "outputs": [],
   "source": [
    "# Generate the attribution\n",
    "expt_attribution = metadata_store_pb2.Attribution()\n",
    "expt_attribution.artifact_id = schema_artifact_id\n",
    "expt_attribution.context_id = expt_context_id\n",
    "\n",
    "# Generate the association\n",
    "expt_association = metadata_store_pb2.Association()\n",
    "expt_association.execution_id = dv_execution_id\n",
    "expt_association.context_id = expt_context_id\n",
    "\n",
    "# Submit attribution and association to the Metadata Store\n",
    "store.put_attributions_and_associations([expt_attribution], [expt_association])\n",
    "\n",
    "print('Experiment Attribution:\\n', expt_attribution)\n",
    "print('Experiment Association:\\n', expt_association)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving Information from the Metadata Store\n",
    "\n",
    "You've now recorded the needed information to the metadata store. If we did this in a persistent database, you can track which artifacts and events are related to each other even without seeing the code used to generate it. See a sample run below where you investigate what dataset is used to generate the schema. (**It would be obvious which dataset is used in our simple demo because we only have two artifacts registered. Thus, assume that you have thousands of entries in the metadata store.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get artifact types\n",
    "store.get_artifact_types()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 1st element in the list of `Schema` artifacts.\n",
    "# You will investigate which dataset was used to generate it.\n",
    "schema_to_inv = store.get_artifacts_by_type('Schema')[0]\n",
    "\n",
    "# print output\n",
    "print(schema_to_inv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get events related to the schema id\n",
    "schema_events = store.get_events_by_artifact_ids([schema_to_inv.id])\n",
    "\n",
    "print(schema_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that it is an output of an execution so you can look up the execution id to see related artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get events related to the output above\n",
    "execution_events = store.get_events_by_execution_ids([schema_events[0].execution_id])\n",
    "\n",
    "print(execution_events)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see the declared input of this execution so you can select that from the list and lookup the details of the artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look up the artifact that is a declared input\n",
    "artifact_input = execution_events[0]\n",
    "\n",
    "store.get_artifacts_by_id([artifact_input.artifact_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now you've fetched the dataset artifact that was used to generate the schema. You can approach this differently and we urge you to practice using the different methods of the [MetadataStore API](https://www.tensorflow.org/tfx/ml_metadata/api_docs/python/mlmd/MetadataStore) to get more familiar with interacting with the database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrap Up\n",
    "\n",
    "In this notebook, you got to practice using ML Metadata outside of TFX. This should help you understand its inner workings so you will know better how to query ML Metadata stores or even set it up for your own use cases. TFX leverages this library to keep records of pipeline runs and you will get to see more of that in the next labs. Next up, you will review how to work with schemas and in the next notebook, you will see how it can be implemented in a TFX pipeline."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "L1_MLMetadata_Walkthrough.ipynb",
   "provenance": [
    {
     "file_id": "1qi5soJiI5vJvjx9CWmF6Ox2heEnIxhYK",
     "timestamp": 1602192239657
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
